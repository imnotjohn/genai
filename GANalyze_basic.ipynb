{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZpD04SX6rRu",
        "outputId": "988d8256-7469-4172-fd11-baa138146d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GANalyze'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 57 (delta 1), reused 4 (delta 1), pack-reused 50\u001b[K\n",
            "Unpacking objects: 100% (57/57), 864.46 KiB | 4.57 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/LoreGoetschalckx/GANalyze.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd GANalyze/pytorch\n",
        "!sh download_pretrained.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdz-zwOj7Qmm",
        "outputId": "edc8d16b-bb0e-4ec1-e052-2831918f91ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GANalyze/pytorch\n",
            "Downloading EmoNet weights\n",
            "--2023-03-22 14:29:40--  http://ganalyze.csail.mit.edu/models/EmoNet_valence_moments_resnet50_5_best.pth.tar\n",
            "Resolving ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)... 128.30.100.223\n",
            "Connecting to ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)|128.30.100.223|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 188384043 (180M) [application/x-tar]\n",
            "Saving to: ‘assessors/EmoNet_valence_moments_resnet50_5_best.pth.tar’\n",
            "\n",
            "EmoNet_valence_mome 100%[===================>] 179.66M  5.05MB/s    in 35s     \n",
            "\n",
            "2023-03-22 14:30:16 (5.09 MB/s) - ‘assessors/EmoNet_valence_moments_resnet50_5_best.pth.tar’ saved [188384043/188384043]\n",
            "\n",
            "Downloading BigGAN weights\n",
            "--2023-03-22 14:30:16--  http://ganalyze.csail.mit.edu/models/biggan-128.pth\n",
            "Resolving ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)... 128.30.100.223\n",
            "Connecting to ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)|128.30.100.223|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 282218374 (269M)\n",
            "Saving to: ‘generators/biggan-128.pth’\n",
            "\n",
            "biggan-128.pth      100%[===================>] 269.14M  3.51MB/s    in 89s     \n",
            "\n",
            "2023-03-22 14:31:46 (3.04 MB/s) - ‘generators/biggan-128.pth’ saved [282218374/282218374]\n",
            "\n",
            "--2023-03-22 14:31:46--  http://ganalyze.csail.mit.edu/models/biggan-256.pth\n",
            "Resolving ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)... 128.30.100.223\n",
            "Connecting to ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)|128.30.100.223|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 328932321 (314M)\n",
            "Saving to: ‘generators/biggan-256.pth’\n",
            "\n",
            "biggan-256.pth      100%[===================>] 313.69M  4.59MB/s    in 2m 34s  \n",
            "\n",
            "2023-03-22 14:34:21 (2.03 MB/s) - ‘generators/biggan-256.pth’ saved [328932321/328932321]\n",
            "\n",
            "--2023-03-22 14:34:21--  http://ganalyze.csail.mit.edu/models/biggan-512.pth\n",
            "Resolving ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)... 128.30.100.223\n",
            "Connecting to ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)|128.30.100.223|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329746160 (314M)\n",
            "Saving to: ‘generators/biggan-512.pth’\n",
            "\n",
            "biggan-512.pth      100%[===================>] 314.47M  4.66MB/s    in 1m 45s  \n",
            "\n",
            "2023-03-22 14:36:07 (3.00 MB/s) - ‘generators/biggan-512.pth’ saved [329746160/329746160]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be careful! The below command cleans all previous checkpoints"
      ],
      "metadata": {
        "id": "6KDkAIjOIkoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/GANalyze/pytorch/checkpoints/*"
      ],
      "metadata": {
        "id": "z4-yEOLKCRID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training\n",
        "The below command line runs the train-pytorch.py script to run the training process. Update the num_samples parameter to set number of iterations. Change the checkpoint_resume to continue training on a specified checkpoint.\n",
        "For further information, check out https://github.com/LoreGoetschalckx/GANalyze"
      ],
      "metadata": {
        "id": "nWNLEfGtFmj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_pytorch.py \\\n",
        " --generator biggan256 None \\\n",
        " --assessor emonet \\\n",
        " --transformer OneDirection None \\\n",
        " --train_alpha_a -0.5 --train_alpha_b 0.5 \\\n",
        " --gpu_id 0 --num_samples 2000 --checkpoint_resume 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhofukfY8e95",
        "outputId": "c1721294-7cae-49cb-9749-554322bbd26b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "approach:  one_direction\n",
            "\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "[0/2000] Loss 0.131149 (0.131149)\n",
            "saving checkpoint\n",
            "[4/2000] Loss 0.036781 (0.083965)\n",
            "[8/2000] Loss 0.040570 (0.069500)\n",
            "[12/2000] Loss 0.102708 (0.077802)\n",
            "[16/2000] Loss 0.161770 (0.094595)\n",
            "[20/2000] Loss 0.018938 (0.081986)\n",
            "[24/2000] Loss 0.162383 (0.093471)\n",
            "[28/2000] Loss 0.079837 (0.091767)\n",
            "[32/2000] Loss 0.134422 (0.096506)\n",
            "[36/2000] Loss 0.028711 (0.089727)\n",
            "[40/2000] Loss 0.060338 (0.087055)\n",
            "[44/2000] Loss 0.112123 (0.089144)\n",
            "[48/2000] Loss 0.043444 (0.085629)\n",
            "[52/2000] Loss 0.027143 (0.081451)\n",
            "[56/2000] Loss 0.137553 (0.085191)\n",
            "[60/2000] Loss 0.115322 (0.087074)\n",
            "[64/2000] Loss 0.085983 (0.087010)\n",
            "[68/2000] Loss 0.113234 (0.088467)\n",
            "[72/2000] Loss 0.029761 (0.085377)\n",
            "[76/2000] Loss 0.033193 (0.082768)\n",
            "[80/2000] Loss 0.074507 (0.082375)\n",
            "[84/2000] Loss 0.203287 (0.087871)\n",
            "[88/2000] Loss 0.135228 (0.089930)\n",
            "[92/2000] Loss 0.082601 (0.089624)\n",
            "[96/2000] Loss 0.203917 (0.094196)\n",
            "[100/2000] Loss 0.134702 (0.095754)\n",
            "[104/2000] Loss 0.111092 (0.096322)\n",
            "[108/2000] Loss 0.052180 (0.094746)\n",
            "[112/2000] Loss 0.003847 (0.091611)\n",
            "[116/2000] Loss 0.067059 (0.090793)\n",
            "[120/2000] Loss 0.009136 (0.088159)\n",
            "[124/2000] Loss 0.169925 (0.090714)\n",
            "[128/2000] Loss 0.117053 (0.091512)\n",
            "[132/2000] Loss 0.027784 (0.089638)\n",
            "[136/2000] Loss 0.116756 (0.090412)\n",
            "[140/2000] Loss 0.104760 (0.090811)\n",
            "[144/2000] Loss 0.088024 (0.090736)\n",
            "[148/2000] Loss 0.203362 (0.093700)\n",
            "[152/2000] Loss 0.093399 (0.093692)\n",
            "[156/2000] Loss 0.057834 (0.092795)\n",
            "[160/2000] Loss 0.041968 (0.091556)\n",
            "[164/2000] Loss 0.053920 (0.090660)\n",
            "[168/2000] Loss 0.052358 (0.089769)\n",
            "[172/2000] Loss 0.095405 (0.089897)\n",
            "[176/2000] Loss 0.011156 (0.088147)\n",
            "[180/2000] Loss 0.205095 (0.090690)\n",
            "[184/2000] Loss 0.062310 (0.090086)\n",
            "[188/2000] Loss 0.120095 (0.090711)\n",
            "[192/2000] Loss 0.051587 (0.089912)\n",
            "[196/2000] Loss 0.219299 (0.092500)\n",
            "[200/2000] Loss 0.208631 (0.094777)\n",
            "[204/2000] Loss 0.098744 (0.094854)\n",
            "[208/2000] Loss 0.006888 (0.093194)\n",
            "[212/2000] Loss 0.075219 (0.092861)\n",
            "[216/2000] Loss 0.085855 (0.092734)\n",
            "[220/2000] Loss 0.145173 (0.093670)\n",
            "[224/2000] Loss 0.141162 (0.094503)\n",
            "[228/2000] Loss 0.054825 (0.093819)\n",
            "[232/2000] Loss 0.124553 (0.094340)\n",
            "[236/2000] Loss 0.105747 (0.094530)\n",
            "[240/2000] Loss 0.202868 (0.096306)\n",
            "[244/2000] Loss 0.076037 (0.095979)\n",
            "[248/2000] Loss 0.129446 (0.096510)\n",
            "[252/2000] Loss 0.059983 (0.095940)\n",
            "[256/2000] Loss 0.114095 (0.096219)\n",
            "[260/2000] Loss 0.149240 (0.097022)\n",
            "[264/2000] Loss 0.065630 (0.096554)\n",
            "[268/2000] Loss 0.095928 (0.096545)\n",
            "[272/2000] Loss 0.011423 (0.095311)\n",
            "[276/2000] Loss 0.020475 (0.094242)\n",
            "[280/2000] Loss 0.103877 (0.094378)\n",
            "[284/2000] Loss 0.069162 (0.094027)\n",
            "[288/2000] Loss 0.048807 (0.093408)\n",
            "[292/2000] Loss 0.094375 (0.093421)\n",
            "[296/2000] Loss 0.088290 (0.093353)\n",
            "[300/2000] Loss 0.141433 (0.093985)\n",
            "[304/2000] Loss 0.090847 (0.093944)\n",
            "[308/2000] Loss 0.046609 (0.093338)\n",
            "[312/2000] Loss 0.249245 (0.095311)\n",
            "[316/2000] Loss 0.070424 (0.095000)\n",
            "[320/2000] Loss 0.034391 (0.094252)\n",
            "[324/2000] Loss 0.244587 (0.096085)\n",
            "[328/2000] Loss 0.073725 (0.095816)\n",
            "[332/2000] Loss 0.068975 (0.095496)\n",
            "[336/2000] Loss 0.021502 (0.094626)\n",
            "[340/2000] Loss 0.040026 (0.093991)\n",
            "[344/2000] Loss 0.109170 (0.094165)\n",
            "[348/2000] Loss 0.092682 (0.094148)\n",
            "[352/2000] Loss 0.045121 (0.093598)\n",
            "[356/2000] Loss 0.026235 (0.092849)\n",
            "[360/2000] Loss 0.017242 (0.092018)\n",
            "[364/2000] Loss 0.067375 (0.091750)\n",
            "[368/2000] Loss 0.153846 (0.092418)\n",
            "[372/2000] Loss 0.093243 (0.092427)\n",
            "[376/2000] Loss 0.085346 (0.092352)\n",
            "[380/2000] Loss 0.065551 (0.092073)\n",
            "[384/2000] Loss 0.098528 (0.092140)\n",
            "[388/2000] Loss 0.141036 (0.092639)\n",
            "[392/2000] Loss 0.103386 (0.092747)\n",
            "[396/2000] Loss 0.072672 (0.092546)\n",
            "[400/2000] Loss 0.067067 (0.092294)\n",
            "[404/2000] Loss 0.020630 (0.091592)\n",
            "[408/2000] Loss 0.047370 (0.091162)\n",
            "[412/2000] Loss 0.090028 (0.091151)\n",
            "[416/2000] Loss 0.104657 (0.091280)\n",
            "[420/2000] Loss 0.042068 (0.090816)\n",
            "[424/2000] Loss 0.103183 (0.090931)\n",
            "[428/2000] Loss 0.040388 (0.090463)\n",
            "[432/2000] Loss 0.075639 (0.090327)\n",
            "[436/2000] Loss 0.003122 (0.089534)\n",
            "[440/2000] Loss 0.072697 (0.089383)\n",
            "[444/2000] Loss 0.075880 (0.089262)\n",
            "[448/2000] Loss 0.096109 (0.089323)\n",
            "[452/2000] Loss 0.086387 (0.089297)\n",
            "[456/2000] Loss 0.094924 (0.089346)\n",
            "[460/2000] Loss 0.008258 (0.088647)\n",
            "[464/2000] Loss 0.071613 (0.088501)\n",
            "[468/2000] Loss 0.103890 (0.088632)\n",
            "[472/2000] Loss 0.086558 (0.088614)\n",
            "[476/2000] Loss 0.039498 (0.088205)\n",
            "[480/2000] Loss 0.098633 (0.088291)\n",
            "[484/2000] Loss 0.033673 (0.087844)\n",
            "[488/2000] Loss 0.045809 (0.087502)\n",
            "[492/2000] Loss 0.117091 (0.087740)\n",
            "[496/2000] Loss 0.049037 (0.087431)\n",
            "[500/2000] Loss 0.091423 (0.087462)\n",
            "[504/2000] Loss 0.084781 (0.087441)\n",
            "[508/2000] Loss 0.027151 (0.086970)\n",
            "[512/2000] Loss 0.080582 (0.086921)\n",
            "[516/2000] Loss 0.031333 (0.086493)\n",
            "[520/2000] Loss 0.071010 (0.086375)\n",
            "[524/2000] Loss 0.058675 (0.086165)\n",
            "[528/2000] Loss 0.062966 (0.085991)\n",
            "[532/2000] Loss 0.003356 (0.085374)\n",
            "[536/2000] Loss 0.160153 (0.085928)\n",
            "[540/2000] Loss 0.027665 (0.085500)\n",
            "[544/2000] Loss 0.034409 (0.085127)\n",
            "[548/2000] Loss 0.048357 (0.084860)\n",
            "[552/2000] Loss 0.071412 (0.084763)\n",
            "[556/2000] Loss 0.124168 (0.085045)\n",
            "[560/2000] Loss 0.086693 (0.085057)\n",
            "[564/2000] Loss 0.051054 (0.084817)\n",
            "[568/2000] Loss 0.033488 (0.084458)\n",
            "[572/2000] Loss 0.129750 (0.084773)\n",
            "[576/2000] Loss 0.124592 (0.085047)\n",
            "[580/2000] Loss 0.043462 (0.084763)\n",
            "[584/2000] Loss 0.078102 (0.084717)\n",
            "[588/2000] Loss 0.083110 (0.084706)\n",
            "[592/2000] Loss 0.062640 (0.084558)\n",
            "[596/2000] Loss 0.032934 (0.084214)\n",
            "[600/2000] Loss 0.112272 (0.084400)\n",
            "[604/2000] Loss 0.161708 (0.084909)\n",
            "[608/2000] Loss 0.099955 (0.085007)\n",
            "[612/2000] Loss 0.114566 (0.085199)\n",
            "[616/2000] Loss 0.069385 (0.085097)\n",
            "[620/2000] Loss 0.007639 (0.084600)\n",
            "[624/2000] Loss 0.154980 (0.085049)\n",
            "[628/2000] Loss 0.057720 (0.084876)\n",
            "[632/2000] Loss 0.135793 (0.085196)\n",
            "[636/2000] Loss 0.027960 (0.084838)\n",
            "[640/2000] Loss 0.137788 (0.085167)\n",
            "[644/2000] Loss 0.029378 (0.084823)\n",
            "[648/2000] Loss 0.079845 (0.084792)\n",
            "[652/2000] Loss 0.164447 (0.085278)\n",
            "[656/2000] Loss 0.081907 (0.085257)\n",
            "[660/2000] Loss 0.078899 (0.085219)\n",
            "[664/2000] Loss 0.146061 (0.085583)\n",
            "[668/2000] Loss 0.057222 (0.085415)\n",
            "[672/2000] Loss 0.073024 (0.085341)\n",
            "[676/2000] Loss 0.090115 (0.085369)\n",
            "[680/2000] Loss 0.077328 (0.085322)\n",
            "[684/2000] Loss 0.081723 (0.085301)\n",
            "[688/2000] Loss 0.165981 (0.085768)\n",
            "[692/2000] Loss 0.126772 (0.086003)\n",
            "[696/2000] Loss 0.194231 (0.086622)\n",
            "[700/2000] Loss 0.118583 (0.086803)\n",
            "[704/2000] Loss 0.053899 (0.086617)\n",
            "[708/2000] Loss 0.152120 (0.086985)\n",
            "[712/2000] Loss 0.134434 (0.087251)\n",
            "[716/2000] Loss 0.073678 (0.087175)\n",
            "[720/2000] Loss 0.046762 (0.086952)\n",
            "[724/2000] Loss 0.068442 (0.086850)\n",
            "[728/2000] Loss 0.179340 (0.087356)\n",
            "[732/2000] Loss 0.033011 (0.087060)\n",
            "[736/2000] Loss 0.077041 (0.087006)\n",
            "[740/2000] Loss 0.080047 (0.086969)\n",
            "[744/2000] Loss 0.126634 (0.087181)\n",
            "[748/2000] Loss 0.033385 (0.086895)\n",
            "[752/2000] Loss 0.066012 (0.086784)\n",
            "[756/2000] Loss 0.084450 (0.086772)\n",
            "[760/2000] Loss 0.018507 (0.086414)\n",
            "[764/2000] Loss 0.073342 (0.086346)\n",
            "[768/2000] Loss 0.050236 (0.086159)\n",
            "[772/2000] Loss 0.143856 (0.086457)\n",
            "[776/2000] Loss 0.134107 (0.086701)\n",
            "[780/2000] Loss 0.043865 (0.086482)\n",
            "[784/2000] Loss 0.111686 (0.086610)\n",
            "[788/2000] Loss 0.134169 (0.086851)\n",
            "[792/2000] Loss 0.023655 (0.086533)\n",
            "[796/2000] Loss 0.164440 (0.086923)\n",
            "[800/2000] Loss 0.023650 (0.086608)\n",
            "[804/2000] Loss 0.058461 (0.086468)\n",
            "[808/2000] Loss 0.122295 (0.086645)\n",
            "[812/2000] Loss 0.124384 (0.086830)\n",
            "[816/2000] Loss 0.004488 (0.086428)\n",
            "[820/2000] Loss 0.202135 (0.086990)\n",
            "[824/2000] Loss 0.043050 (0.086778)\n",
            "[828/2000] Loss 0.052262 (0.086612)\n",
            "[832/2000] Loss 0.094027 (0.086647)\n",
            "[836/2000] Loss 0.075692 (0.086595)\n",
            "[840/2000] Loss 0.064959 (0.086493)\n",
            "[844/2000] Loss 0.074133 (0.086434)\n",
            "[848/2000] Loss 0.039799 (0.086215)\n",
            "[852/2000] Loss 0.089144 (0.086229)\n",
            "[856/2000] Loss 0.072035 (0.086163)\n",
            "[860/2000] Loss 0.053368 (0.086011)\n",
            "[864/2000] Loss 0.119122 (0.086164)\n",
            "[868/2000] Loss 0.158350 (0.086495)\n",
            "[872/2000] Loss 0.076721 (0.086450)\n",
            "[876/2000] Loss 0.039894 (0.086239)\n",
            "[880/2000] Loss 0.072346 (0.086176)\n",
            "[884/2000] Loss 0.043218 (0.085982)\n",
            "[888/2000] Loss 0.030662 (0.085734)\n",
            "[892/2000] Loss 0.120026 (0.085887)\n",
            "[896/2000] Loss 0.031848 (0.085647)\n",
            "[900/2000] Loss 0.082237 (0.085632)\n",
            "[904/2000] Loss 0.091478 (0.085658)\n",
            "[908/2000] Loss 0.102849 (0.085733)\n",
            "[912/2000] Loss 0.145208 (0.085993)\n",
            "[916/2000] Loss 0.142425 (0.086238)\n",
            "[920/2000] Loss 0.157481 (0.086547)\n",
            "[924/2000] Loss 0.020317 (0.086261)\n",
            "[928/2000] Loss 0.099776 (0.086319)\n",
            "[932/2000] Loss 0.160326 (0.086635)\n",
            "[936/2000] Loss 0.089346 (0.086647)\n",
            "[940/2000] Loss 0.109030 (0.086742)\n",
            "[944/2000] Loss 0.057542 (0.086619)\n",
            "[948/2000] Loss 0.083855 (0.086607)\n",
            "[952/2000] Loss 0.145059 (0.086852)\n",
            "[956/2000] Loss 0.067202 (0.086770)\n",
            "[960/2000] Loss 0.201991 (0.087248)\n",
            "[964/2000] Loss 0.076482 (0.087203)\n",
            "[968/2000] Loss 0.150190 (0.087462)\n",
            "[972/2000] Loss 0.170758 (0.087804)\n",
            "[976/2000] Loss 0.095169 (0.087834)\n",
            "[980/2000] Loss 0.074723 (0.087781)\n",
            "[984/2000] Loss 0.166378 (0.088099)\n",
            "[988/2000] Loss 0.112821 (0.088199)\n",
            "[992/2000] Loss 0.084487 (0.088184)\n",
            "[996/2000] Loss 0.045334 (0.088012)\n",
            "[1000/2000] Loss 0.119223 (0.088137)\n",
            "saving checkpoint\n",
            "[1004/2000] Loss 0.108487 (0.088217)\n",
            "[1008/2000] Loss 0.094968 (0.088244)\n",
            "[1012/2000] Loss 0.150712 (0.088490)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GANalyze/pytorch/train_pytorch.py\", line 189, in <module>\n",
            "    gan_images_transformed = input_transform(utils.pytorch.denorm(gan_images_transformed))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n",
            "    img = t(img)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\", line 471, in __call__\n",
            "    return self.lambd(img)\n",
            "  File \"/content/GANalyze/pytorch/assessors/emonet.py\", line 49, in <lambda>\n",
            "    torchvision.transforms.Lambda(lambda image: tencrop(image.permute(0, 2, 3, 1), cropped_size=224)),\n",
            "  File \"/content/GANalyze/pytorch/assessors/emonet.py\", line 80, in tencrop\n",
            "    crops[img_index, curr, :, :, :] = temp_img.permute(2, 0, 1)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training script for your reference (You DO NOT need to run this block if you have trained your model through the above command line). However, feel free to play with it and see how other generator + assessor combinations can be experimented."
      ],
      "metadata": {
        "id": "zdrVVIc4CbJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import assessors\n",
        "import generators\n",
        "import transformations.pytorch as transformations\n",
        "import utils.common\n",
        "import utils.pytorch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Collect command line arguments\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--gpu_id', type=str, default=0, help='which gpu to use.')\n",
        "parser.add_argument('--num_samples', type=int, default=\"400000\", help='number of samples to train for')\n",
        "parser.add_argument('--checkpoint_resume', type=int, default=0, help='which checkpoint to load based on batch_start. -1 for latest stored checkpoint')\n",
        "parser.add_argument('--train_alpha_a', type=float, default=-0.5, help='lower limit for step sizes to use during training')\n",
        "parser.add_argument('--train_alpha_b', type=float, default=0.5, help='upper limit for step sizes to use during training')\n",
        "parser.add_argument('--generator', default=[\"biggan256\", \"None\"], nargs=2, type=str, metavar=[\"name\", \"arguments\"], help='generator function to use')\n",
        "parser.add_argument('--assessor', type=str, default=\"emonet\", help='assessor function to compute the image property of interest')\n",
        "parser.add_argument('--transformer', default=[\"OneDirection\", \"None\"], nargs=2, type=str, metavar=[\"name\", \"arguments\"], help=\"transformer function\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "opts = vars(args)\n",
        "\n",
        "# Verify\n",
        "if opts[\"checkpoint_resume\"] != 0 and opts[\"checkpoint_resume\"] != -1:\n",
        "    assert(opts[\"checkpoint_resume\"] % 4 == 0)  # Needs to be a multiple of the batch size\n",
        "\n",
        "# Choose GPU\n",
        "if opts[\"gpu_id\"] != -1:\n",
        "    device = torch.device(\"cuda:\" + str(opts[\"gpu_id\"]) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Creating directory to store checkpoints\n",
        "version = subprocess.check_output([\"git\", \"describe\", \"--always\"]).strip().decode(\"utf-8\")\n",
        "checkpoint_dir = os.path.join(\n",
        "    \"./checkpoints\",\n",
        "    \"_\".join(opts[\"generator\"]),\n",
        "    opts[\"assessor\"],\n",
        "    \"_\".join(opts[\"transformer\"]),\n",
        "    version)\n",
        "\n",
        "if opts[\"checkpoint_resume\"] == 0:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=False)\n",
        "\n",
        "# Saving training settings\n",
        "opts_file = os.path.join(checkpoint_dir, \"opts.json\")\n",
        "opts[\"version\"] = version\n",
        "with open(opts_file, 'w') as fp:\n",
        "    json.dump(opts, fp)\n",
        "\n",
        "# Setting up file to store loss values\n",
        "loss_file = os.path.join(checkpoint_dir, \"losses.txt\")\n",
        "\n",
        "# Some characteristics\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "dim_z = {\n",
        "    'biggan256': 140,\n",
        "    'biggan512': 128\n",
        "}.get(opts['generator'][0])\n",
        "\n",
        "vocab_size = {'biggan256': 1000, 'biggan512': 1000}.get(opts['generator'][0])\n",
        "\n",
        "# Setting up Transformer\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "transformer = opts[\"transformer\"][0]\n",
        "transformer_arguments = opts[\"transformer\"][1]\n",
        "if transformer_arguments != \"None\":\n",
        "    key_value_pairs = transformer_arguments.split(\",\")\n",
        "    key_value_pairs = [pair.split(\"=\") for pair in key_value_pairs]\n",
        "    transformer_arguments = {pair[0]: pair[1] for pair in key_value_pairs}\n",
        "else:\n",
        "    transformer_arguments = {}\n",
        "\n",
        "transformation = getattr(transformations, transformer)(dim_z, vocab_size, **transformer_arguments)\n",
        "transformation = transformation.to(device)\n",
        "\n",
        "# Setting up Generator\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "generator = opts[\"generator\"][0]\n",
        "generator_arguments = opts[\"generator\"][1]\n",
        "if generator_arguments != \"None\":\n",
        "    key_value_pairs = generator_arguments.split(\",\")\n",
        "    key_value_pairs = [pair.split(\"=\") for pair in key_value_pairs]\n",
        "    generator_arguments = {pair[0]: pair[1] for pair in key_value_pairs}\n",
        "else:\n",
        "    generator_arguments = {}\n",
        "\n",
        "generator = getattr(generators, generator)(**generator_arguments)\n",
        "\n",
        "for p in generator.parameters():\n",
        "    p.requires_grad = False\n",
        "generator.eval()\n",
        "generator = generator.to(device)\n",
        "\n",
        "# Setting up Assessor\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "assessor_elements = getattr(assessors, opts['assessor'])(True)\n",
        "if isinstance(assessor_elements, tuple):\n",
        "    assessor = assessor_elements[0]\n",
        "    input_transform = assessor_elements[1]\n",
        "    output_transform = assessor_elements[2]\n",
        "else:\n",
        "    assessor = assessor_elements\n",
        "\n",
        "    def input_transform(x):\n",
        "        return x  # identity, no preprocessing\n",
        "\n",
        "    def output_transform(x):\n",
        "        return x  # identity, no postprocessing\n",
        "\n",
        "if hasattr(assessor, 'parameters'):\n",
        "    for p in assessor.parameters():\n",
        "        p.requires_grad = False\n",
        "        assessor.eval()\n",
        "        assessor.to(device)\n",
        "\n",
        "# Training\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "# optimizer\n",
        "optimizer = optim.Adam(transformation.parameters(), lr=0.0002)\n",
        "losses = utils.common.AverageMeter(name='Loss')\n",
        "\n",
        "# figure out where to resume\n",
        "if opts[\"checkpoint_resume\"] == 0:\n",
        "    checkpoint_resume = 0\n",
        "elif opts[\"checkpoint_resume\"] == -1:\n",
        "    available_checkpoints = [x for x in os.listdir(checkpoint_dir) if x.endswith(\".pth\")]\n",
        "    available_batch_numbers = [x.split('.')[0].split(\"_\")[-1] for x in available_checkpoints]\n",
        "    latest_number = max(available_batch_numbers)\n",
        "    file_to_load = available_checkpoints[available_batch_numbers.index(latest_number)]\n",
        "    transformation.load_state_dict(torch.load(os.path.join(checkpoint_dir, file_to_load)))\n",
        "    checkpoint_resume = latest_number\n",
        "else:\n",
        "    transformation.load_state_dict(torch.load(os.path.join(checkpoint_dir,\n",
        "                                                           \"pytorch_model_{}.pth\".format(opts[\"checkpoint_resume\"]))))\n",
        "    checkpoint_resume = opts[\"checkpoint_resume\"]\n",
        "\n",
        "#  training settings\n",
        "optim_iter = 0\n",
        "batch_size = 4\n",
        "train_alpha_a = opts[\"train_alpha_a\"]\n",
        "train_alpha_b = opts[\"train_alpha_b\"]\n",
        "num_samples = opts[\"num_samples\"]\n",
        "\n",
        "# create training set\n",
        "np.random.seed(seed=0)\n",
        "truncation = 1\n",
        "zs = utils.common.truncated_z_sample(num_samples, dim_z, truncation)\n",
        "ys = np.random.randint(0, vocab_size, size=zs.shape[0])\n",
        "\n",
        "# loop over data batches\n",
        "for batch_start in range(0, num_samples, batch_size):\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # skip batches we've already done (this would happen when resuming from a checkpoint)\n",
        "    if batch_start <= checkpoint_resume and checkpoint_resume != 0:\n",
        "        optim_iter = optim_iter + 1\n",
        "        continue\n",
        "\n",
        "    # input batch\n",
        "    s = slice(batch_start, min(num_samples, batch_start + batch_size))\n",
        "    z = torch.from_numpy(zs[s]).type(torch.FloatTensor).to(device)\n",
        "    y = torch.from_numpy(ys[s]).to(device)\n",
        "    step_sizes = (train_alpha_b - train_alpha_a) * \\\n",
        "        np.random.random(size=(batch_size)) + train_alpha_a  # sample step_sizes\n",
        "    step_sizes_broadcast = np.repeat(step_sizes, dim_z).reshape([batch_size, dim_z])\n",
        "    step_sizes_broadcast = torch.from_numpy(step_sizes_broadcast).type(torch.FloatTensor).to(device)\n",
        "\n",
        "    # ganalyze steps\n",
        "    gan_images = generator(z, utils.pytorch.one_hot(y))\n",
        "    # save sample images here\n",
        "    img = gan_images\n",
        "    gan_images = input_transform(utils.pytorch.denorm(gan_images))\n",
        "    gan_images = gan_images.view(-1, *gan_images.shape[-3:])\n",
        "    gan_images = gan_images.to(device)\n",
        "    out_scores = output_transform(assessor(gan_images)).to(device).float()\n",
        "    target_scores = out_scores + torch.from_numpy(step_sizes).to(device).float()\n",
        "\n",
        "    z_transformed = transformation.transform(z, utils.pytorch.one_hot(y), step_sizes_broadcast)\n",
        "    gan_images_transformed = generator(z_transformed, utils.pytorch.one_hot(y))\n",
        "    gan_images_transformed = input_transform(utils.pytorch.denorm(gan_images_transformed))\n",
        "    gan_images_transformed = gan_images_transformed.view(-1, *gan_images_transformed.shape[-3:])\n",
        "    gan_images_transformed = gan_images_transformed.to(device)\n",
        "    out_scores_transformed = output_transform(assessor(gan_images_transformed)).to(device).float()\n",
        "\n",
        "    # compute loss\n",
        "    loss = transformation.compute_loss(out_scores_transformed, target_scores, batch_start, loss_file)\n",
        "\n",
        "    # backwards\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print loss\n",
        "    losses.update(loss.item(), batch_size)\n",
        "    print(f'[{batch_start}/{num_samples}] {losses}')\n",
        "\n",
        "    if optim_iter % 50 == 0:\n",
        "        print(\"saving checkpoint\")\n",
        "        torch.save(transformation.state_dict(), os.path.join(checkpoint_dir, \"pytorch_model_{}.pth\".format(batch_start)))\n",
        "        # plot sample images\n",
        "        img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "        img_np = img.detach().cpu().numpy().squeeze()\n",
        "\n",
        "        fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
        "        for i in range(4): # By defualt batchsize = 4\n",
        "            row = i // 2\n",
        "            col = i % 2\n",
        "            ax[row, col].imshow(img_np[i])\n",
        "        plt.savefig(os.path.join(checkpoint_dir, \"sample_image_{}.png\".format(batch_start)))\n",
        "        plt.show()\n",
        "\n",
        "    optim_iter = optim_iter + 1\n",
        "\n",
        "torch.save(transformation.state_dict(), os.path.join(checkpoint_dir, \"pytorch_model_{}.pth\".format(opts[\"num_samples\"])))\n"
      ],
      "metadata": {
        "id": "qlZiiEyh_8RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing\n",
        "Use the below command line to run the testing phase and generate the interpolation results. Change the paramters to specify the checkpoint to use."
      ],
      "metadata": {
        "id": "N9xxDn76GTDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf output/*"
      ],
      "metadata": {
        "id": "xpRmbTdD2wTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_pytorch.py \\\n",
        "--alpha 0.1 --test_truncation 1 \\\n",
        "--checkpoint_dir /content/GANalyze/pytorch/checkpoints/biggan256_None/emonet/OneDirection_None/45d4139 \\\n",
        "--checkpoint 1000 \\\n",
        "--gpu_id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZkZiaZffPpD",
        "outputId": "754b13e5-df2f-44e7-ade0-35b2a6a31850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'gpu_id': '0', 'alpha': 0.1, 'test_truncation': 1.0, 'checkpoint_dir': '/content/GANalyze/pytorch/checkpoints/biggan256_None/emonet/OneDirection_None/45d4139', 'checkpoint': 1000, 'mode': 'bigger_step'}\n",
            "\n",
            "approach:  one_direction\n",
            "\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "bigger_step\n",
            "y:  100\n",
            "bigger_step\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GANalyze/pytorch/test_pytorch.py\", line 301, in <module>\n",
            "    ims.append(ims_batch)\n",
            "AttributeError: 'numpy.ndarray' object has no attribute 'append'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, testing script for your reference (You DO NOT need to run this block if you have run testing through the above command line)."
      ],
      "metadata": {
        "id": "bpRtE-6aI8Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "import numpy as np\n",
        "import PIL.ImageDraw\n",
        "import PIL.ImageFont\n",
        "import torch\n",
        "\n",
        "import assessors\n",
        "import generators\n",
        "import transformations.pytorch as transformations\n",
        "import utils.common\n",
        "import utils.pytorch\n",
        "\n",
        "# Collect command line arguments\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--gpu_id', type=str, default=0, help='which gpu to use.')\n",
        "parser.add_argument('--alpha', type=float, default=0.1, help='stepsize for testing')\n",
        "parser.add_argument('--test_truncation', type=float, default=1, help='truncation to use in test phase')\n",
        "parser.add_argument('--checkpoint_dir', type=str, default=\"\", help='path for directory with the checkpoints of the trained model we want to use')\n",
        "parser.add_argument('--checkpoint', type=int, default=400000, help='which checkpoint to load')\n",
        "parser.add_argument('--mode', default=\"bigger_step\", choices=[\"iterative\", \"bigger_step\"],\n",
        "                    help=\"how to make the test sequences. bigger_step was used in the paper.\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "opts = vars(args)\n",
        "print(opts)\n",
        "\n",
        "# Choose GPU\n",
        "if opts[\"gpu_id\"] != -1:\n",
        "    device = torch.device(\"cuda:\" + str(opts[\"gpu_id\"]) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Creating directory to store output visualizations\n",
        "train_opts_file = os.path.join(opts[\"checkpoint_dir\"], \"opts.json\")\n",
        "with open(train_opts_file) as f:\n",
        "    train_opts = json.load(f)\n",
        "\n",
        "if not isinstance(train_opts[\"transformer\"], list):\n",
        "    train_opts[\"transformer\"] = [train_opts[\"transformer\"]]\n",
        "\n",
        "test_version = str(subprocess.check_output([\"git\", \"describe\", \"--always\"]).strip())\n",
        "result_dir = os.path.join(\"./output\",\n",
        "                          \"-\".join(train_opts[\"generator\"]),\n",
        "                          train_opts[\"assessor\"],\n",
        "                          \"-\".join(train_opts[\"transformer\"]),\n",
        "                          train_opts[\"version\"],\n",
        "                          \"alpha_\" + str(opts[\"alpha\"]) + \"_truncation_\" + str(opts[\"test_truncation\"]) + \"_iteration_\" + str(opts[\"checkpoint\"]) + \"_\" + opts[\"mode\"])\n",
        "\n",
        "os.makedirs(result_dir, exist_ok=False)\n",
        "\n",
        "# checkpoint_dir\n",
        "checkpoint_dir = opts[\"checkpoint_dir\"]\n",
        "\n",
        "# Saving testing settings\n",
        "opts_file = os.path.join(result_dir, \"opts.json\")\n",
        "opts[\"test_version\"] = test_version\n",
        "with open(opts_file, 'w') as fp:\n",
        "    json.dump(opts, fp)\n",
        "\n",
        "# Some characteristics\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "dim_z = {\n",
        "    'biggan256': 140,\n",
        "    'biggan512': 128\n",
        "}.get(train_opts['generator'][0])\n",
        "\n",
        "vocab_size = {'biggan256': 1000, 'biggan512': 1000}.get(train_opts['generator'][0])\n",
        "categories_file = \"./generators/categories_imagenet.txt\"\n",
        "categories = [x.strip() for x in open(categories_file)]\n",
        "\n",
        "# Setting up Transformer\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "transformer = train_opts[\"transformer\"][0]\n",
        "transformer_arguments = train_opts[\"transformer\"][1]\n",
        "if transformer_arguments != \"None\":\n",
        "    key_value_pairs = transformer_arguments.split(\",\")\n",
        "    key_value_pairs = [pair.split(\"=\") for pair in key_value_pairs]\n",
        "    transformer_arguments = {pair[0]: pair[1] for pair in key_value_pairs}\n",
        "else:\n",
        "    transformer_arguments = {}\n",
        "\n",
        "transformation = getattr(transformations, transformer)(dim_z, vocab_size, **transformer_arguments)\n",
        "transformation = transformation.to(device)\n",
        "\n",
        "# Setting up Generator\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "generator = train_opts[\"generator\"][0]\n",
        "generator_arguments = train_opts[\"generator\"][1]\n",
        "if generator_arguments != \"None\":\n",
        "    key_value_pairs = generator_arguments.split(\",\")\n",
        "    key_value_pairs = [pair.split(\"=\") for pair in key_value_pairs]\n",
        "    generator_arguments = {pair[0]: pair[1] for pair in key_value_pairs}\n",
        "else:\n",
        "    generator_arguments = {}\n",
        "\n",
        "generator = getattr(generators, generator)(**generator_arguments)\n",
        "\n",
        "for p in generator.parameters():\n",
        "    p.requires_grad = False\n",
        "generator.eval()\n",
        "generator = generator.to(device)\n",
        "\n",
        "# Setting up Assessor\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "assessor_elements = getattr(assessors, train_opts['assessor'])(True)\n",
        "if isinstance(assessor_elements, tuple):\n",
        "    assessor = assessor_elements[0]\n",
        "    input_transform = assessor_elements[1]\n",
        "    output_transform = assessor_elements[2]\n",
        "else:\n",
        "    assessor = assessor_elements\n",
        "\n",
        "    def input_transform(x): return x  # identity, no preprocessing\n",
        "\n",
        "    def output_transform(x): return x  # identity, no postprocessing\n",
        "\n",
        "if hasattr(assessor, 'parameters'):\n",
        "    for p in assessor.parameters():\n",
        "        p.requires_grad = False\n",
        "        assessor.eval()\n",
        "        assessor.to(device)\n",
        "\n",
        "# Testing\n",
        "# --------------------------------------------------------------------------------------------------------------\n",
        "# Figure out where to resume\n",
        "if opts[\"checkpoint\"] == 0:\n",
        "    checkpoint = 0\n",
        "elif opts[\"checkpoint\"] == -1:\n",
        "    available_checkpoints = [x for x in os.listdir(checkpoint_dir) if x.endswith(\".pth\")]\n",
        "    available_batch_numbers = [x.split('.')[0].split(\"_\")[-1] for x in available_checkpoints]\n",
        "    latest_number = max(available_batch_numbers)\n",
        "    file_to_load = available_checkpoints[available_batch_numbers.index(latest_number)]\n",
        "    transformation.load_state_dict(torch.load(os.path.join(checkpoint_dir, file_to_load)))\n",
        "    checkpoint = latest_number\n",
        "else:\n",
        "    transformation.load_state_dict(torch.load(os.path.join(checkpoint_dir,\n",
        "                                                           \"pytorch_model_\" + str(opts[\"checkpoint\"]) + \".pth\")))\n",
        "    checkpoint = opts[\"checkpoint\"]\n",
        "\n",
        "# helper function\n",
        "\n",
        "\n",
        "def make_image(z, y, step_size, transform):\n",
        "    if transform:\n",
        "        z_transformed = transformation.transform(z, y, step_size)\n",
        "        z_transformed = z.norm() * z_transformed / z_transformed.norm()\n",
        "        z = z_transformed\n",
        "\n",
        "    gan_images = utils.pytorch.denorm(generator(z, y))\n",
        "    gan_images_np = gan_images.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
        "    gan_images = input_transform(gan_images)\n",
        "    gan_images = gan_images.view(-1, *gan_images.shape[-3:])\n",
        "    gan_images = gan_images.to(device)\n",
        "\n",
        "    out_scores_current = output_transform(assessor(gan_images))\n",
        "    out_scores_current = out_scores_current.detach().cpu().numpy()\n",
        "    if len(out_scores_current.shape) == 1:\n",
        "        out_scores_current = np.expand_dims(out_scores_current, 1)\n",
        "\n",
        "    return(gan_images_np, z, out_scores_current)\n",
        "\n",
        "\n",
        "# Test settings\n",
        "num_samples = 10\n",
        "truncation = opts[\"test_truncation\"]\n",
        "iters = 3\n",
        "np.random.seed(seed=999)\n",
        "annotate = True\n",
        "\n",
        "if vocab_size == 0:\n",
        "    num_categories = 1\n",
        "else:\n",
        "    # set to 1 for debugging\n",
        "    num_categories = 1 #vocab_size\n",
        "\n",
        "for y in range(num_categories):\n",
        "\n",
        "    ims = []\n",
        "    outscores = []\n",
        "\n",
        "    zs = utils.common.truncated_z_sample(num_samples, dim_z, truncation)\n",
        "    ys = np.repeat(y, num_samples)\n",
        "    zs = torch.from_numpy(zs).type(torch.FloatTensor).to(device)\n",
        "    ys = torch.from_numpy(ys).to(device)\n",
        "    ys = utils.pytorch.one_hot(ys, vocab_size)\n",
        "    step_sizes = np.repeat(np.array(opts[\"alpha\"]), num_samples * dim_z).reshape([num_samples, dim_z])\n",
        "    step_sizes = torch.from_numpy(step_sizes).type(torch.FloatTensor).to(device)\n",
        "    feed_dicts = []\n",
        "    for batch_start in range(0, num_samples, 4):\n",
        "        s = slice(batch_start, min(num_samples, batch_start + 4))\n",
        "        feed_dicts.append({\"z\": zs[s], \"y\": ys[s], \"truncation\": truncation, \"step_sizes\": step_sizes[s]})\n",
        "\n",
        "    for feed_dict in feed_dicts:\n",
        "        ims_batch = []\n",
        "        outscores_batch = []\n",
        "        z_start = feed_dict[\"z\"]\n",
        "        step_sizes = feed_dict[\"step_sizes\"]\n",
        "\n",
        "        if opts[\"mode\"] == \"iterative\":\n",
        "            print(\"iterative\")\n",
        "\n",
        "            # original seed image\n",
        "            x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=False)\n",
        "            x = np.uint8(x)\n",
        "            if annotate:\n",
        "                ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
        "            else:\n",
        "                if annotate:\n",
        "                    ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
        "                else:\n",
        "                    ims_batch.append(x)\n",
        "            outscores_batch.append(outscore)\n",
        "\n",
        "            # negative clone images\n",
        "            z_next = z_start\n",
        "            step_sizes = -step_sizes\n",
        "            for iter in range(0, iters, 1):\n",
        "                feed_dict[\"step_sizes\"] = step_sizes\n",
        "                feed_dict[\"z\"] = z_next\n",
        "                x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n",
        "                x = np.uint8(x)\n",
        "                z_next = tmp\n",
        "                if annotate:\n",
        "                    ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
        "                else:\n",
        "                    if annotate:\n",
        "                        ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
        "                    else:\n",
        "                        ims_batch.append(x)\n",
        "                outscores_batch.append(outscore)\n",
        "\n",
        "            ims_batch.reverse()\n",
        "\n",
        "            # positive clone images\n",
        "            step_sizes = -step_sizes\n",
        "            z_next = z_start\n",
        "            for iter in range(0, iters, 1):\n",
        "                feed_dict[\"step_sizes\"] = step_sizes\n",
        "                feed_dict[\"z\"] = z_next\n",
        "\n",
        "                x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n",
        "                x = np.uint8(x)\n",
        "                z_next = tmp\n",
        "\n",
        "                if annotate:\n",
        "                    ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
        "                else:\n",
        "                    ims_batch.append(x)\n",
        "                outscores_batch.append(outscore)\n",
        "\n",
        "        else:\n",
        "            print(\"bigger_step\")\n",
        "\n",
        "            # original seed image\n",
        "            x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=False)\n",
        "            x = np.uint8(x)\n",
        "            if annotate:\n",
        "                ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
        "            else:\n",
        "                ims_batch.append(x)\n",
        "            outscores_batch.append(outscore)\n",
        "\n",
        "            # negative clone images\n",
        "            step_sizes = -step_sizes\n",
        "            for iter in range(0, iters, 1):\n",
        "                feed_dict[\"step_sizes\"] = step_sizes * (iter + 1)\n",
        "\n",
        "                x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n",
        "                x = np.uint8(x)\n",
        "\n",
        "                if annotate:\n",
        "                    ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
        "                else:\n",
        "                    ims_batch.append(x)\n",
        "                outscores_batch.append(outscore)\n",
        "\n",
        "            ims_batch.reverse()\n",
        "            outscores_batch.reverse()\n",
        "\n",
        "            # positive clone images\n",
        "            step_sizes = -step_sizes\n",
        "            for iter in range(0, iters, 1):\n",
        "                feed_dict[\"step_sizes\"] = step_sizes * (iter + 1)\n",
        "\n",
        "                x, tmp, outscore = make_image(feed_dict[\"z\"], feed_dict[\"y\"], feed_dict[\"step_sizes\"], transform=True)\n",
        "                x = np.uint8(x)\n",
        "                if annotate:\n",
        "                    ims_batch.append(utils.common.annotate_outscore(x, outscore))\n",
        "                else:\n",
        "                    ims_batch.append(x)\n",
        "                outscores_batch.append(outscore)\n",
        "\n",
        "        ims_batch = [np.expand_dims(im, 0) for im in ims_batch]\n",
        "        ims_batch = np.concatenate(ims_batch, axis=0)\n",
        "        ims_batch = np.transpose(ims_batch, (1, 0, 2, 3, 4))\n",
        "        ims.append(ims_batch)\n",
        "\n",
        "        outscores_batch = [np.expand_dims(outscore, 0) for outscore in outscores_batch]\n",
        "        outscores_batch = np.concatenate(outscores_batch, axis=0)\n",
        "        outscores_batch = np.transpose(outscores_batch, (1, 0, 2))\n",
        "        outscores.append(outscores_batch)\n",
        "\n",
        "    ims = np.concatenate(ims, axis=0)\n",
        "    outscores = np.concatenate(outscores, axis=0)\n",
        "    ims_final = np.reshape(ims, (ims.shape[0] * ims.shape[1], ims.shape[2], ims.shape[3], ims.shape[4]))\n",
        "    I = PIL.Image.fromarray(utils.common.imgrid(ims_final, cols=iters * 2 + 1))\n",
        "    I.save(os.path.join(result_dir, categories[y] + \".jpg\"))\n",
        "    print(\"y: \", y)\n"
      ],
      "metadata": {
        "id": "NTbac4bvI2RB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}